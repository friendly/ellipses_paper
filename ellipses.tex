\documentclass[11pt]{article}%\RequirePackage{snapshot}    %% to generate the .dep file
\usepackage{times}
\usepackage[comma]{natbib}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{epigraph}
\usepackage{comment}
\usepackage{mdwlist}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}        %% LaTeX symbols: \Box, etc.th}
\usepackage{color}

\bibliographystyle{abbrvnat-apa-nourl}

\usepackage{afterpage}
\usepackage{fancyhdr}

\newif\iftth
\newcommand*{\figref}[1]{Figure~\ref{#1}}
\newcommand*{\tabref}[1]{Table~\ref{#1}}
\newcommand*{\secref}[1]{Section~\ref{#1}}
\renewcommand*{\eqref}[1]{Eqn.~(\ref{#1})}
\newcommand{\keywords}[1]{\par\noindent\textbf{Key words:} #1}
\newcommand*{\todo}[1]{\marginpar{ToDo:\small{#1}}}
\newcommand{\TODO}[1]{\begin{quotation}\color{blue}\textbf{ToDo}: #1\end{quotation}}


%  Page dimensions
\addtolength{\hoffset}{-2cm}
\addtolength{\textwidth}{4cm}
%\addtolength{\voffset}{-2cm}
\addtolength{\textheight}{2cm}

% Math stuff
% math stuff
\renewcommand*{\vec}[1]{\ensuremath{\bm{#1}}}
%\newcommand{\trans}{\ensuremath{'}}
\newcommand{\trans}{\ensuremath{^\mathsf{T}}}
\newcommand*{\mat}[1]{\ensuremath{\bm{#1}}}
\newcommand*{\diag}[1]{\ensuremath{\mathrm{diag}\, #1}}
\renewcommand*{\det}[1]{\ensuremath{\mathrm{det} (#1)}}
\newcommand*{\rank}[1]{\ensuremath{\mathrm{rank} (\mat{#1})}}
\newcommand*{\trace}[1]{\ensuremath{\mathrm{tr} (\mat{#1})}}
\newcommand*{\dev}[1]{(#1 - \bar{#1})}
\newcommand*{\inv}[1]{\ensuremath{\mat{#1}^{-1}}}
\newcommand*{\half}[1]{\ensuremath{\mat{#1}^{1/2}}}
\newcommand*{\invhalf}[1]{\ensuremath{\mat{#1}^{-1/2}}}
\newcommand*{\nvec}[2]{\ensuremath{{#1}_{1}, {#1}_{2},\ldots,{#1}_{#2}}}
\newcommand*{\Beta}{B}
\newcommand*{\Epsilon}{E}
\newcommand*{\period}{\:\: .}
\newcommand*{\comma}{\:\: ,}
\newcommand*{\given}{\ensuremath{\, | \,}}
\newcommand*{\Real}[1]{\mathbb{R}^{#1}}
\newcommand*{\degree}[1]{\ensuremath{{#1}^{\circ}}}
\newcommand{\sizedmat}[2]{%
  \mathord{\mathop{\mat{#1}}\limits_{(#2)}}%
}
\newcommand{\Var}{\ensuremath{\mathsf{Var}}}
\newcommand{\Cov}{\ensuremath{\mathsf{Cov}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}


% abbreviations
\newcommand{\nway} {\emph{n}-way}
\newcommand{\loglin} {log-linear}
\newcommand{\MLM}{MvLM}

% \fig{name}{includegraphicsoptions}{caption}
\newcommand{\fig}[3]{%
\begin{figure}[!htb]
  \centering%
  \includegraphics[#2]{fig/#1}%
  \caption{#3}%
  \label{fig:#1}
\end{figure}
}

\setlength{\epigraphwidth}{.9\textwidth}

\begin{document}
\begin{titlepage}
\title{Elliptic Insights: Understanding Statistical Methods through Elliptic Geometry}

\author{Michael Friendly%
%\thanks{Michael Friendly is Professor, Psychology Department,
%York University, Toronto, ON, M3J 1P3 Canada, E-mail: friendly@yorku.ca.}
 \\ York University
\and
Georges Monette \\ York University
%\and
%John Fox \\ McMaster University
}
\date{\today}
\end{titlepage}
\maketitle
%\thispagestyle{fancy}\lhead{In press: \emph{Journal of Computational and Statistical Graphics}, 2001}\rhead{}\afterpage{\lhead{}\rhead{}}

\begin{abstract}
Visual insights into  a wide variety  of statistical methods,  for both didactic
and data analytic purposes can often be achieved through geometric diagrams  and
geometrically-based statistical graphs.  This  paper extols and illustrates  the
virtues  of  the  ellipse  and her  higher-dimensional  cousins  for  both these
purposes in a variety of contexts, including general linear models, multivariate
linear models and mixed models. 
We emphasize the strong relations among statistical methods, matrix algebraic
solutions and geometry that can often be easily understood in terms of
ellipses.
%Most of the statistical methods we discuss stem,
%at least implicitly, from classic Gaussian assumptions. But, to whatever extent the
%We survey a wide number of statistical problems in which 

\keywords{Galton; 
Bayesian estimation;
added variable plots; 
concentration ellipse; 
data ellipse; 
discriminant analysis;
hypothesis-error plots;
mixed models;
regression paradoxes;
statistical geometry; 
ridge regression; 
}
\end{abstract}

\section{Introduction}

\epigraph{Whatever relates to extent and quantity may be represented by
geometrical figures. Statistical projections which speak to the senses without
fatiguing the mind, possess the advantage of fixing the attention on a great
number of important facts.}{Alexander von Humboldt \citeyearpar{Humboldt:1811a}, p. ciii}

In the beginning (of modern  statistical methods), there was the  ellipse. As statistical
methods progressed from bivariate to multivariate, the ellipse escaped the plane to a 3D
ellipsoid, and then onwards to higher dimensions.
This  paper extols and illustrates the
virtues of the ellipse and her higher-dimensional cousins for both didactic and
data analytic purposes.

When
Francis Galton  \citeyearpar{Galton:1886} first  studied the  relation between  heritable traits of
parents and their offspring, he  had a remarkable visual insight---  contours of
equal bivariate frequencies in the joint distribution seemed to form  concentric
shapes whose outlines  were, to Galton,  tolerably close to  concentric ellipses
differing only in scale.  

Galton's goal was to  to predict (or explain)  how a characteristic, Y,   (e.g.,
height) of children was  related to that of  their parents, X.  To  this end, he
calculated summaries,  Ave. ($Y\given X$),  and, for  symmetry, Ave.  ($X\given Y$), and plotted
these as lines of means on his  diagram.  Lo and behold, he had a  second visual
insight:  the lines  of means of  ($Y\given X$) and ($X\given Y$)  corresponded approximately to
the locus of  horizontal and vertical  tangents to the  concentric ellipses.  To
complete the picture,  he added lines  showing the major  and minor axes  of the
family of ellipses, with the result shown in Figure 1.

It is  not stretching  the point  too far  to say  that a  large part  of modern
statistical  methods  descend  from  these  visual  insights:%
\footnote{\citet[p. 37]{Pearson:1920} later stated, ``that Galton
should have evolved all this from his observations is to my mind one
of the most noteworthy scientific discoveries arising from pure
analysis of observations.'' }
correlation   and
regression \citep{Pearson:1896}, the  bivariate normal  distribution,
principal components  \citep{Pearson:1901,Hotelling:1933}  all descend  from Galton's  geometrical
diagram.%
\footnote{
Well, not entirely. Auguste Bravais [1811--1863] \citeyearpar{Bravais:1846}, an astronomer
and physicist first introduced the mathematical theory of the bivariate normal distribution
as a model for the joint frequency of errors in the geometric position of a point.
Bravais derived the formula for level slices as concentric ellipses and had a rudimentary
notion of correlation but did not appreciate this as a representation of data.
Nonetheless, \cite{Pearson:1920} acknowledged Bravais' contribution and the correlation
coefficient is often called the Bravais-Pearson coefficient in France
\citep{Denis:2001}. }
%\todo{Ref: Denis}


Basic geometry goes back to Euclid, but the properties of the ellipse and  other
conic sections may be traced to Apollonius  of Perga (ca. 262 BC--–ca. 190 BC),  a
Greek geometer and astronomer who gave the ellipse, parabola and hyperbola their
modern names. In a work popularly called the Conics \citep{Boyer:91}, he  described
the  fundamental  properties  of  ellipses  (eccentricity,  axes,  principles of
tangency,  normals as  minimum and  maximum straight  lines to  the curve)  with
remarkable clarity nearly 2000 years before the development of analytic geometry
by Descartes.

% one figure
\begin{figure}[htb]
  \centering
  \includegraphics[width=.75\textwidth]{fig/galton-corr}
  \caption{Galton's 1886 diagram, showing the relation of height of children
to the average of their parents' height. The diagram is essentially an overlay
of a geometrical interpretation on a bivariate grouped frequency distribution, shown
as numbers.}%
  \label{fig:galton-corr}
\end{figure}

Over time, the ellipse would be called to duty to provide simple explanations of
phenomena  once thought  complex.  Most  notable is  Kepler’s insight  that the
Copernican theory of the orbits of planets as concentric circles (which required
notions  of  epicycles  to  account  for  observations)  could  be  brought into
alignment with the detailed observations by  Tycho Brahe and others by a  simple
law: ``The orbit  of every planet  is an ellipse  with the sun  at a focus.''  One
century later, Isaac  Newton was able  to derive all  three of Kepler's  laws as
simpler consequences of general laws of motion and universal gravitation.

This paper  takes up  the cause  of the  ellipse as  a geometric  form that  can
provide similar service to statistical understanding and data analysis.  Indeed,
it has been doing that since the time of Galton, but these graphic and geometric
contributions have often  been incidental and  scattered in the  literature.  We
focus here on visual  insights through ellipses in  the areas of linear  models,
multivariate models and mixed models.


\begin{comment}
%% two figures side-by-side
\begin{figure}[htb]
 \begin{minipage}[b]{.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{fig/}
  \caption{}%
  \label{fig:}
 \end{minipage}%
 \hfill
 \begin{minipage}[b]{.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{fig/}
  \caption{}
  \label{fig:}
 \end{minipage}
\end{figure}
\end{comment}

\section{Notation and basic results}
There are various representations of an ellipse (or ellipsoid in three or more dimensions),
both geometric and statistical.  
To avoid repeating ``or ellipsoid'' we use ``ellipsoid'' as generic where context is clear. 
We make use of the following definitions and results in what follows for geometric and
statistical properties.
\subsection{Geometrical ellipsoids}
A general unit ellipsoid in the $p$-dimensional space $\Real{p}$
%$\Re^p$
centered at the origin, $\vec{0}$,
may be defined by the quadratic form
\begin{equation}\label{eq:ellisoid1}
\mathcal{E} := \{ \vec{x}: \vec{x}\trans \mat{C} \vec{x} =1 \}
\end{equation}
where $\vec{x} = (x_1, x_2, \dots , x_p)\trans$ is a vector referring to the coordinate axes and $\mat{C}$ is a symmetric 
non-negative definite $p \times p$ matrix. Some useful properties are:
\begin{figure}[tb]
  \centering
  \includegraphics[width=.5\textwidth,clip]{fig/inverse}
  \caption{Some properties of geometric ellipsoids. Principal axes of an ellipsoid are given by the eigenvectors of
  $\mat{C}$, with radii $\sqrt{\lambda_i}$.  For a standard unit ellipsoid defined by \eqref{eq:ellisoid1},
  the comparable ellipsoid for $2\mat{C}$ has radii multiplied by $\sqrt{2}$.
  The ellipsoid for $\mat{C}^{-1}$ has the same principle axes, but with radii $1/\sqrt{\lambda_i}$, making it
  small in the directions where $\mat{C}$ is large and vice-versa.
  }%
  \label{fig:inverse}
\end{figure}

\begin{itemize}
 \item Translation: The unit ellipsoid centered at $\vec{x}_0$ has the equation $\mathcal{E} := \{ \vec{x}: (\vec{x}-\vec{x}_0)\trans \mat{C} (\vec{x}-\vec{x}_0) =1 \}$.
 \item Orthogonality: If $\mat{C}$ is diagonal, the origin-centered unit ellipsoid has its axes aligned with the coordinate axes, and
has the equation
\begin{equation}\label{eq:ellisoid2}
 \vec{x}\trans \mat{C} \vec{x} = c_{11} x_1^2 + c_{22} x_2^2 + \cdots + c_{pp} x_p^2 =1
\end{equation}
where $1/\sqrt{c_{ii}} = c_{ii}^{-1/2}$ are the radii (semi-diameter lengths) along the coordinate axes.
 \item Area and volume: In two dimensions, the area of the axis-aligned ellipse is $\pi (c_{11} c_{22})^{-1/2}$.
 For $p=3$, the volume is $\frac{4}{3}\pi (c_{11} c_{22} c_{33})^{-1/2}$.
 In the general case, the hypervolume of the ellipsoid is proportional to $|\mat{C}|^{-1/2}$
 and is given by $\pi^{p/2} / \det{\mat{C}}^{1/2} \Gamma(\frac{p}{2}+1)$.
%\todo{Is there a general formula for hypervolume in $p$D?}
 \item Principal axes: In general, the eigenvectors, $\vec{v_i}, i=1,\dots,p$,
of $\mat{C}$ define the principal axes of the ellipsoid and
the inverse of the square roots of the ordered
eigenvalues, $\lambda_1 > \lambda_2 \dots, \lambda_p$, are the principal radii.
 \item Inverse: Since the eigenvectors of \mat{C} and $\mat{C}^{-1}$ are identical, while the
 eigenvalues of $\mat{C}^{-1}$ are $1/\lambda_i$, it follows that the ellipsoid for
 $\mat{C}^{-1}$ has the same axes as that of $\mat{C}$, but with inversely proportional radii.
 In $\Real{2}$ (with appropriate scaling of the axes), the ellipsoid for $\mat{C}^{-1}$
 is thus a \degree{90} rotation of the ellipsoid for $\mat{C}$,
 as illustrated in \figref{fig:inverse}.
 \item Dimensionality: The ellipsoid is only $p$-dimensional if $\mat{C}$ is positive definite (all $\lambda_i > 0$).
 Each $\lambda_i = 0$ reduces dimensionality by one.  For example, with $p=3$, $\lambda_3=0$ gives a
2D ellipse in 3-space, and  $\lambda_2 = \lambda_3=0$ gives a degenerate line.
 \item Projections: The projection of a $p$ dimensional ellisoid into any subspace
is $\vec{x}\trans (\mat{P}\mat{C}\mat{P}\trans ) \vec{x} =1$, where
$\mat{P}$ is an idempotent $p \times p$ matrix, i.e., $\mat{P} \mat{P}= \mat{P}^2 = \mat{P}$.
For example, in $\Real{2}$ and $\Real{3}$,
% $\Re^3$, 
the matrices
\[
\mat{P}_2 =
\left[
\begin{array}{cc}
 1 & 1  \\
 0 & 0  \\
\end{array}
\right]
\comma \quad\quad 
\mat{P}_3 =
\left[
\begin{array}{ccc}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 0 \\
\end{array}
\right]
\]
project, respectively, an ellipse onto the line $x_1 = x_2$, and an ellipsoid into the ($x_1, x_2$) plane.
 \item Slopes and tangents: The slopes of the ellipsoidal surface in the directions of the coordinate
 axes are given by $\partial / \partial \vec{x} \: (\vec{x}\trans \mat{C} \vec{x}) = 2 \mat{C} \vec{x}$.
 From this, it follows that the tangent hyperplane to the unit ellopsoidal surface at the point
 $\vec{x}_\alpha$, where $\vec{x}_\alpha\trans \: \partial / \partial \vec{x} (\vec{x}\trans \mat{C} \vec{x}) = 0$,
 has the equation $\vec{x}_\alpha \trans \mat{C} \vec{x} = 1$.
\end{itemize}

\subsection{Statistical ellipsoids}
 
In statistical applications, $\mat{C}$ will often be the inverse of a covariance
matrix (or a sum of squares and cross-products matrix), and the ellipsoid will
be centered at the means of variables, or at estimates of parameters under some model.
Hence, we will also use the following notations:

For a positive definite matrix 
$\mat{\Sigma}$ we use $E(\vec{\mu},\mat{\Sigma})$ to denote the ellipsoid 
\begin{equation}\label{eq:ellipsoid3}
\mathcal{E} = \{ \vec{x} : (\vec{x}-\vec{\mu})\trans \inv{\Sigma} (x-\vec{\mu}) = 1 \} \period
 \end{equation} 

When $\mat{\Sigma}$ is the covariance matrix of a multivariate vector $\vec{x}$ with eigenvalues
$\lambda_1 > \lambda_2 > \dots$,
the following
properties represent the ``size'' of the ellipsoid in $\Real{p}$:

\begin{tabular}{llll}
    Size                   &  Conceptual formula                    & Geometry       & Function \\
\hline
(a) Generalized variance:  & $\det{\mat{\Sigma}} = \prod^p \lambda_i$ & area, volume & geometric mean\\
(b) Average variance:        & $\trace{\mat{\Sigma}} = \sum^p \lambda_i $ & linear sum & arithmetic mean\\
(c) Average variance:        & $1/ \trace{\mat{\Sigma}^{-1}} = 1/\sum^p (1/\lambda_i) $ &  & harmonic mean\\
(d) Maximal variance:      & $\lambda_1$ & maximum dimension & supremum
 \end{tabular}
\medskip

In multivariate tests, these correspond (with suitable transformations) to (a) Wilks' $\Lambda$,
(b) Pillai trace criteria, (c) Hotelling-Lawley   and (d) Roy's maximum root test, as we describe
below.

Note that every non-negative definite matrix $\mat{W}$ can be factored as $\mat{W}=\mat{A}\mat{A}\trans$,
and the matrix $\mat{A}$ can always be selected so that it is square. 
$\mat{A}$ will be non-singular if and only if $\mat{W}$ is non-singular.  
A computational
definition of an ellipsoid that can be used for all non-negative definite matrices and that corresponds to the previous definition in the case of positive-definite matrices is
\begin{equation}
E(\vec{\mu},\mat{W}) = \vec{\mu} + \mat{A} \mathcal{S} \comma
\end{equation}
where $\mathcal{S}$ is a unit sphere of conformable dimension and $\vec{\mu}$ is the centroid of the ellipsoid.
One convenient choice of \mat{A} is the Choleski square root, $\mat{W}^{1/2}$ as we describe in \secref{sec:conjugate}.
Thus, for some results described below, a convenient notation in terms of $\mat{W}$ is
\begin{equation}\label{eq:ellipsoidW}
E(\vec{\mu}, \mat{W}) = \vec{\mu} \oplus \sqrt{\mat{W}} = \vec{\mu} \oplus \mat{W}^{1/2} \comma
\end{equation} 
where $\oplus$
emphasizes that the ellipsoid is a scaling and rotation of the unit sphere followed by translation to
a center at \vec{\mu} and $\sqrt{\mat{W}}=\mat{W}^{1/2}=\mat{A}$. This representation is not unique;
however,  $\vec{\mu} \oplus \mat{B} = \vec{\nu} \oplus \mat{C}$ (they generate the same ellipsoid)
\emph{iff} $\vec{\mu} = \vec{\nu}$ and $\mat{B}\mat{B}\trans = \mat{C}\mat{C}\trans$.

From this, it is readily seen that under a linear transformation given by a matrix
$\mat{L}$
the image of the ellipse is:

\begin{equation*}
\mat{L}(E(\vec{\mu} ,\mat{W}))=E(\mat{L}\vec{\mu} ,\mat{L}\mat{W}{\mat{L}}\trans)=\mat{L}\vec{\mu} \oplus \sqrt{\mat{L}\mat{W}{\mat{L}}\trans}=\mat{L}\vec{\mu} \oplus \mat{L}\sqrt{\mat{W}}
\end{equation*}

\input{conjugate}

\subsection{Ellipsoids in a generalized metric space}
\todo{Georges?}
\TODO{
Smooth out and simplify this description.
Add to the plot: unit vectors in data space, and their transformations in canonical space.
%Here it would be useful to describe the geometric relations that pertain to \mat{H} 
%(positive semi-definite)
%in the metric of \mat{E} (positive definite).
%I see two sub-figures:  
%(a) ellipses for \mat{H} and \mat{E}, showing the conjugate axes and
%bounding parallelogram for \mat{E}, perhaps using the principal component factorization.
%(b) the transformation of (a) using $\mat{H}\mat{E}^{-1}$ or
%$\mat{E}^{-1/2} \, \mat{H} \, \mat{E}^{-1/2}$.
}

In the discussion above, we considered the positive semi-definite matrix \mat{W} and
corresponding ellipsoid to be 
referred to a Euclidean space, perhaps with different basis vectors. 
We showed that various measures of the ``size'' of the ellipsoid could be defined
in terms of functions of the eigenvalues $\lambda_i$ of \mat{W}.

We now consider the generalized 
case of an analogous $p \times p$ positive semi-definite symmetric matrix \mat{H}, but where measures of
length, distance and angles are referred to a metric defined by a positive definite symmetric
matrix \mat{E}. As is well known, the generalized eigenvalue problem is to find the scalars
$\lambda_i$ and vectors $\vec{v}_i, i=1, 2, \dots p$,
such that $\mat{H} \vec{v} = \lambda \mat{E} \vec{v}$, that is, the roots of
$\det{\mat{H} - \lambda \mat{E}}=0$.

%\TODO{More explanation here ...}
\begin{figure}[htb]
% two figs side-by-side
  \begin{minipage}[c]{.495\textwidth}
   \includegraphics[width=1\linewidth,clip]{fig/ellipse-geneig1}
   \end{minipage}%
  \hfill
  \begin{minipage}[c]{.495\textwidth}
   \includegraphics[width=1\linewidth,clip]{fig/ellipse-geneig2}
  \end{minipage}
  \caption{Left: Ellipses for \mat{H} and \mat{E} in Euclidean ``data space''. 
   Right: Ellipses for $\mat{H}^\star$ and $\mat{E}^\star$ in the transformed ``canonical space'',
   with the eivenvectors of\mat{H} relative to \mat{E} shown as blue arrows, whose radii
   are the corresponding eigenvalues, $\lambda_1, \lambda_2$. }%
  \label{fig:ellipse-geneig}
\end{figure}
For such \mat{H} and \mat{E}, we can always find a factor \mat{A} of \mat{E}, so that
$\mat{E} = \mat{A} \mat{A}\trans$, whose colums will be conjugate directions for \mat{E}
and whose rows will also be conjugate directions for \mat{H}, in that $\mat{H} = \mat{A}\trans \mat{D} \mat{A}$,
where $\mat{D}$ is diagonal.  Geometrically, this means that there exists a unique pair of
bounding parallelograms for the \mat{H} and \mat{E} ellipsoids whose 
corresponding sides are parallel. A linear transformation of \mat{E} and \mat{H}
that transforms the parallelogram
for \mat{E} to a square (or cuboid), and hence \mat{E} to a spheroid, generates an
equivalent view in what we describe below as canonical space.


In statistical applications (e.g., MANOVA, canonical correlation), the generalized
eigenvalue problem is transformed to an ordinary eigenvalue problem by considering
the equivalent forms with the same $\lambda_i$, $\vec{v}_i$,
\begin{eqnarray*}
(\mat{H} - \lambda \mat{E}) \vec{v} & = & \vec{0} \\ 
\Rightarrow (\mat{H} \, \inv{\mat{E}} - \lambda \mat{I}) \vec{v} & = & \vec{0} \\
\Rightarrow (\invhalf{\mat{E}} \, \mat{H} \, \invhalf{\mat{E}} - \lambda \mat{I}) \vec{v} & = & \vec{0}
\end{eqnarray*}
where the last form gives a symmetric matrix, $\mat{H}^\star = \invhalf{E} \, \mat{H} \, \invhalf{E}$.
Using the square root of \mat{E} defined by the
principal component factorization $\half{E} = \mat{\Gamma} \half{\mat{\Lambda}}$ gives 
the ellipsoid of $\mat{H}^\star$
orthogonal axes corresponding to the $\vec{v}_i$, whose radii are the corresponding
eigenvalues $\lambda_i$.  This can be seen geometrically as a rotation of ``data space''
to an orientation defined by the principal axes of \mat{E}, followed by a re-scaling, so
that the \mat{E} ellipsoid becomes the unit sphere.  In this transformed space
(``canonical space''), functions of the 
the radii $\lambda_i$ of the axes of $\mat{H}^\star$ give direct measures of
the ``size'' of \mat{H} relative to \mat{E}. The orientation of the eigenvectors
$\vec{v}_i$ can be related to the (orthogonal) linear combinations of the 
data variables which are successively largest in the metric of \mat{E}.


To illustrate, \figref{fig:ellipse-geneig}(left) shows 
the ellipses generated by
\begin{equation*}
 \mat{H} = \left[ \begin{array}{cc}
                   9 & 3 \\
                   3 & 4
                  \end{array}\right] 
 \quad \mbox{ and } \quad
 \mat{E} = \left[ \begin{array}{cc}
                   1 & 0.5 \\
                   0.5 & 2
                  \end{array}\right] 
\end{equation*}
together with their conjugate axes. For \mat{E}, the conjugate axes are defined by the columns of the right factor,
$\mat{A}\trans$,
in $\mat{E} = \mat{A} \mat{A}\trans$; for \mat{H}, the conjugate axes are defined by columns of $\mat{A}$.
The transformation to $\mat{H}^\star = \invhalf{E} \, \mat{H} \, \invhalf{E}$ is shown in the right panel
of \figref{fig:ellipse-geneig}. In this ``canonical space,'' angles and lengths have the orginary interpretation
of Euclidean space, so the size of $\mat{H}^\star$ can be interpreted directly in terms of functions of
the radii $\lambda_1$ and $\lambda_2$.


\section{The data ellipse and ellipsoids}\label{sec:data-ellipse}
The \emph{data ellipse} \citep{Monette:90} (or \emph{concentration ellipse} \citet[Ch. 7]{Dempster:69})
provides a remarkably
simple and effective display for viewing and understanding
bivariate \emph{marginal} relationships in multivariate data.
It is typically used to add a visual summary to a scatterplot,
indicating the means, standard deviations, correlation,
and slope of the regression line for
two variables. Under classical (Gaussian) assumptions, the data ellipse
provides a sufficient visual summary, as we describe below.


\begin{figure}[htb]
  \centering
  \includegraphics[width=.9\textwidth,clip]{fig/galton-reg3}
  \caption{Sunflower plot of Galton's data on heights of parents and their children (in.), with
  40\%, 68\% and 95\% data ellipses and the regression lines of $y$ on $x$ (black) and
  $x$ on $y$ (grey). The ratio of the vertical to the regression line (labeled `r') to the vertical
  to the top of the ellipse gives a visual estimate of the correlation ($r$=0.46, here).
  Shadows (projections) on the coordinate axes give standard intervals,
  $\bar{x}_i \pm s_i$, with various coverage properties.
  Plotting children's height on the abscissa follows Galton.
  }%
  \label{fig:galton-reg3}
\end{figure}

It is historically appropriate to illustrate the data ellipse and
describe its properties using Galton's (\citeyear[Table
I]{Galton:1886}) actual
data, from which he drew \figref{fig:galton-corr} as a conceptual
diagram,%
\footnote{These data are reproduced in \citet[Table 8.2, p.
286]{Stigler:1986}}
shown in \figref{fig:galton-reg3}, where the frequency at
each point is shown by a sunflower symbol. We also overlay the 40\%,
68\% and 95\% data ellipses, as described below.

In \figref{fig:galton-reg3} the ellipses have the mean vector
$(\bar{x}, \bar{y})$ as their center;  the lengths of arms of the
central cross show the standard deviation of each variable, which
may be seen to correspond to the shadows of the 40\% ellipse.  In
addition, the correlation coefficient may be visually estimated as
the fraction of a vertical tangent line from $\bar{y}$ to the top of
the ellipse that is below the regression line $\hat{y} | x$, shown
by the arrow labeled `r.' Finally, as Galton noted, the regression line
for $\hat{y} \given x$ (or $\hat{x} \given y$)
can be visually estimated as the locus of the points of vertical
(or horizontal) tangents with the family of concentric ellipses.
See \citet[Fig. 5.1--5.2]{Monette:90} and
\citet[p. 183]{Friendly:91} for illustrations and further discussion
of the properties of the data ellipse.

More formally \citep{Dempster:69,Monette:90}, for a $p$-dimensional
sample, $\mat{Y}_{n \times p}$, 
we recognize the quadratic form in \eqref{eq:ellipsoid3}
as corresponding to the squared Mahalanobis distance,
$D^2_M (\vec{y}) = \dev{\vec{y}}\trans \, \inv{S} \, \dev{\vec{y}}$
of the point 
$\vec{y} = (y_1, y_2, \dots , y_p)\trans$ 
from the centroid of the sample,
$\bar{\vec{y}} = (\bar{y_1}, \bar{y_2}, \dots , \bar{y_p})\trans$.
Thus, we define the the data ellipsoid $\mathcal{E}_c$ of size (``radius'') $c$
as the set of all points $\vec{y}$ with $D^2_M (\vec{y})$ less than or
equal to $c^2$,
\begin{equation}\label{eq:dsq}
\mathcal{E}_c ( \vec{y}; \mat{S},  \bar{\vec{y}} ) 
\equiv \{ \vec{y} : 
\dev{\vec{y}}\trans \, \inv{S} \, \dev{\vec{y}} \le c^2 \} \comma
\end{equation}
where $\mat{S}$ is the sample variance-covariance matrix,
\(
\mat{S}
 = ({n-1})^{-1} \sum_{i=1}^n (\vec{y}_i - \bar{\vec{y}})\trans (\vec{y}_i - \bar{\vec{y}})
% \period
\).  In the computational notation of \eqref{eq:ellipsoidW}, the boundary of the
data ellipsoid of radius $c$ is thus
\begin{equation}\label{eq:ellipsoidS}
E_c(\bar{\vec{y}}, \mat{S}) = \bar{\vec{y}} \oplus c \mat{S}^{1/2} \period
\end{equation} 

Many properties of the data ellipsoid hold regardless of the joint distribution of the
variables; but if the variables are multivariate normal, then the data ellipsoid represents
a contour of constant density in their joint distribution.  In this case $D^2_M (\vec{y})$
has a large-sample $\chi^2_p$ distribution (or, in finite samples, approximately
$[p (n-1) / (n-p)] F_{p, n-p}$)).

Hence, in the bivariate case, taking $c^2 = \chi^2_2(0.95)= 5.99 \approx 6$ encloses approximately
95\% of the data points under normal theory.  Other radii also have useful interpretations:
\begin{itemize*}
\item In \figref{fig:galton-reg3}, we demonstrate that $c^2 = \chi^2_2(0.40) \approx 1$ gives
a data ellipse of 40\% coverage with the property that its projection on either axis
corresponds to a standard interval, $\bar{y} \pm 1 s$.  The same property of univariate
coverage pertains to
any linear combination of $y_1$ and $y_2$.
\item By analogy with a univariate sample, a 68\% coverage data ellipse with
$c^2 = \chi^2_2(0.68) = 2.28$ gives a bivariate analog of the standard $\bar{y} \pm 1 s$ interval.
The univariate shadows, or those of any linear combination, then correspond to standard
intervals taking fishing in a $p=2$-dimensional space into account.
\end{itemize*}


\begin{figure}[htb]
% two figs side-by-side
  \begin{minipage}[c]{.485\textwidth}
   \includegraphics[width=1\linewidth,clip]{fig/scatirisd1}
   \end{minipage}%
  \hfill
  \begin{minipage}[c]{.485\textwidth}
   \includegraphics[width=1\linewidth,clip]{fig/scatirisd3}
  \end{minipage}
  \caption{Scatterplot matrices of Anderson's iris data: (a) showing data, separate 68\% data
  ellipses and regression lines for each species; (b) showing only ellipses and regression lines.
  Key-- \emph{Iris setosa}: blue, $\triangle$s; \emph{Iris versicolor}: red, $+$;
  \emph{Iris virginca}: green, $\Box$.}%
  \label{fig:scatirisd1}
\end{figure}

As useful as the data ellipse might be for a single, unstructured
sample, its value as a visual summary increases
with the complexity of the data.
For example, \figref{fig:scatirisd1} shows  scatterplot matrices
of all pairwise plots of the variables from Edgar Anderson's \citeyear{Anderson:35}
classic
data on three species of iris flowers found in the Gasp\'{e} Peninsula,
later used by \citet{Fisher:36} his development of discriminant analysis.
The data ellipses show clearly that the means, variances, correlations,
and regression slopes differ systematically across the three iris species
in all pairwise plots.  
We emphasize that these serve as sufficient visual summaries of the important
statistical properties (first and second moments) by removing the data points
from the plots in the version at the right.

\subsection{Robust data ellipsoids}
We recognize that a normal-theory summary (first and second moments),
shown visually or numerically, can be distorted
by multivariate outliers, particularly in smaller samples.
Such effects can be countered by using
robust covariance
estimates such as multivariate trimming \citep{GnanadesikanKettenring:72}
or the high-breakdown bound Minimum Volume Ellipsoid (MVE) and
Minimum Covariance Determinant (MCD) methods
developed by Rousseeuw and others
\citep{RousseeuwLeroy:87,RousseeuwVanDriessen:99}.
In what follows, it should be noted that
robust covariance estimates could, in principle, be substituted
for the classical, normal-theory estimates in all cases.
To save space, we don't explore these possibilities further here.



\section{Linear models: data ellipses and confidence ellipses}
Here we consider how ellipses help to visualize relations among variables
in connection with linear models (regression, ANOVA).
We begin with views in the space of the variables (data space)
and progress to related views in the space of model parameters
($\vec{\beta}$ space).

\begin{comment}
Topics:
\begin{itemize*}
%\item data ellipse as a sufficient visual summary
\item visualizing CIs for slopes and visualizing r
\item conditional/marginal effects and Simpson's paradox
\item confidence ellipse as the representation in beta space
\item VIFs \& GVIFs 
\item Added variable plots: ellipse of AVP as a 2d section
\item other neat relations
\end{itemize*}
\end{comment}

\input{linreg}
\input{simpson-iris}
\input{paradoxes}
\input{levdemo}

\subsection[Ellipsoids in data space and beta space]{Ellipsoids in data space and $\vec{\beta}$ space}
It is most common to look at data and fitted models in ``data space,'' where axes correspond to
variables, points represent observations, and fitted models are plotted as lines (or planes) in this space.
As we've suggested, data ellipsoids provide informative summaries of relations in data space.
For linear models, particularly regression models with quantitative predictors, there is another space---$\vec{\beta}$ space,
that provides deeper views of models and relations among them.
In $\vec{\beta}$ space, the axes pertain to coefficients and points are models (true, hypothesized, fitted) whose coordinates 
represent values of parameters.

In the sense described below, data space and $\vec{\beta}$ space are \emph{dual} to each other.
In simple linear regression, for example, each line in data space corresponds to a point in $\vec{\beta}$ space,
the set of points on any line in $\vec{\beta}$ space corresponds to a pencil of lines through a given point
in data space, and the proposition that every pair of points define a line in one space corresponds to
the proposition that every two lines intersect in a point in the other space.

Moreover, ellipsoids in these spaces are dual and inversely related to each other.
In data space, joint confidence intervals for the mean vector or joint prediction
regions for the data are given by the ellipsoids $\bar{y} \oplus c \sqrt{\mat{S}}$.
In the dual of $\vec{\beta}$ space, joint confidence regions for the parameters
are given by ellipsoids of the form $\hat{\beta} \oplus c \sqrt{\mat{S}^{-1}}$.
We illustrate these relations in the example below.

\begin{figure}[htb]
  \centering
  \includegraphics[width=.6\textwidth,clip]{fig/vis-reg-coffee11}
  \caption{Scatterplot matrix, showing the relations between Heart ($y$), Coffee ($x_1$) and Stress ($x_2$),
  with linear regression lines and 68\% data ellipses for the marginal bivariate relations.
  }%
  \label{fig:vis-reg-coffee11}
\end{figure}

\figref{fig:vis-reg-coffee11} shows a scatterplot matrix among the variables
Heart ($y$): an index of cardiac damage, Coffee ($x_1$): a measure of daily
coffee consumption, and Stress ($x_2$), a measure of occupational stress in a contrived
sample of $n=20$. For the sake of the example we assume that the main goal is
to determine whether or not coffee is good or bad for your heart, and stress
represents one potential confounding variable among others (age, smoking, etc.)
that might be useful to control statistically.

The plot in \figref{fig:vis-reg-coffee11} shows only the marginal relations
between each pair of variables. The marginal message seems to be that coffee is
bad for your heart, stress is bad for your heart and coffee consumption is 
also related to occupational stress.

\begin{figure}[htb]
% two figs side-by-side
  \begin{minipage}[c]{.485\textwidth}
   \includegraphics[width=1\linewidth,clip]{fig/vis-reg-coffee12a}
   \end{minipage}%
  \hfill
  \begin{minipage}[c]{.485\textwidth}
   \includegraphics[width=1\linewidth,clip]{fig/vis-reg-coffee12b}
  \end{minipage}
  \caption{Data space and $\vec{\beta}$ space representations of Coffee and Stress.
   Left: Standard (40\%) data ellipse Right: Joint 95\% confidence ellipse (green) for
   ($\beta_{\mathrm{Coffee}}, \beta_{\mathrm{Stress}}$), CI ellipse (red) with 95\% univariate shadows.
  }%
  \label{fig:vis-reg-coffee12}
\end{figure}

Yet, when we fit both variables together, we obtain the following results,
suggesting that coffee is good for you (the coefficient for coffee is now
negative, though non-significant).  How can this be?

\begin{verbatim}
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -7.7943     5.7927  -1.346    0.196    
Coffee       -0.4091     0.2918  -1.402    0.179    
Stress        1.1993     0.2244   5.345 5.36e-05 ***
---
Signif. codes:  0 '***'’ 0.001 '**'’ 0.01 '*'’ 0.05 ‘'.' 0.1 ' ' 1 

Residual standard error: 10.36 on 17 degrees of freedom
Multiple R-squared: 0.9462,     Adjusted R-squared: 0.9399 
F-statistic: 149.6 on 2 and 17 DF,  p-value: 1.620e-11 
\end{verbatim}

\figref{fig:vis-reg-coffee12} shows the relation between the
predictors in data space and how this translates into joint and
individual confidence intervals for the coefficients in 
$\vec{\beta}$ space.  The left panel is the same as the corresponding
(Coffee, Stress) panel in \figref{fig:vis-reg-coffee11}, but with
a standard (40\%) data ellipse. The right panel shows 95\% confidence
regions and intervals in $\vec{\beta}$ space, determined as
\begin{equation*}
 \widehat{\beta} \oplus \sqrt{d F^{.95}_{q, \nu}} \times s_e \times \mat{S}_X^{-1/2}
\end{equation*}
where $d$ is the number of dimensions for which we want coverage,
$\nu$ is the residual degrees of freedom for $s_e$ and $\mat{S}_X$
is the covariance matrix of the predictors.  

Thus, the green ellipse in \figref{fig:vis-reg-coffee12} is the
ellipse of joint 95\% coverage, using the factor $\sqrt{2 F^{.95}_{2, \nu}}$
and covering the true values of ($\beta_{\mathrm{Stress}}, \beta_{\mathrm{Coffee}}$)
in 95\% of samples.  Moreover:
\begin{itemize*}
  \item Any \emph{joint} hypothesis (e.g., $H_0:\beta_{\mathrm{Stress}}=1, \beta_{\mathrm{Coffee}}=1$)
can be tested visually, simply by observing whether the
hypothesized point, $(1, 1)$ here, lies inside or outside the joint ellipse.
  \item The shadows of this ellipse on the horizontal and vertical axes
give Scheff\'e joint 95\%  confidence intervals for each parameter, with protection for ``fishing''
in a 2-dimensional space.  
  \item Similarly, using the factor
$\sqrt{F^{1-\alpha/d}_{1, \nu}} = t^{1-\alpha/2d}_\nu$ would give an
ellipse whose 1D shadows are $1-\alpha$ Bonferroni confidence intervals
for $d$ posterior hypotheses.
\end{itemize*}

Visual hypothesis tests and $d=1$ confidence intervals for the parameters \emph{separately} 
are obtained from the red ellipse in \figref{fig:vis-reg-coffee12},
which is scaled by $\sqrt{F^{.95}_{1, \nu}} = t^{.975}_\nu$.
The shadows of this ellipse on the axes (thick red lines) give the
corresponding individual 95\% confidence intervals, which are
equivalent to the (partial, Type III) $t$-tests for each coefficient given in the
standard multiple regression output shown above.
Thus, controlling for Stress, the confidence interval for the slope for Coffee includes 0,
so we cannot reject the hypothesis that $\beta_{\mathrm{Coffee}}=0$
in the multiple regression model, as we saw above in the numerical output.
On the other hand, the interval for the slope for Stress excludes the origin,
so we reject the null hypothesis that $\beta_{\mathrm{Stress}}=0$,
controlling for Coffee consumption.

Finally, consider the relation between the data ellipse and the 
confidence ellipse.  These have exactly the same shape, but
(with equal coordinate scaling of the axes), the confidence ellipse
is exactly a $90^o$ rotation of the data ellipse.  In directions in
data space where the data ellipse is wide--- where we have more information
about the relation between Coffee and Stress--- the confidence ellipse is
narrow, reflecting greater precision of the estimates of coefficients.
Conversely, where the data ellipse is narrow (less information), the
confidence ellipse is wide (less precision).

\begin{figure}[htb]
  \centering
  \includegraphics[width=.6\textwidth,clip]{fig/vis-reg-coffee13}
  \caption{Joint 95\% confidence ellipse for ($\beta_{\mathrm{Coffee}}, \beta_{\mathrm{Stress}}$), 
  together with the 1D marginal confidence interval for $\beta_{\mathrm{Coffee}}$
  ignoring Stress (thick blue line), and a visual confidence interval for $\beta_{\mathrm{Stress}} - \beta_{\mathrm{Coffee}}=0$
  (dark cyan).
  }%
  \label{fig:vis-reg-coffee13}
\end{figure}

The virtues of the confidence ellipse for visualizing hypothesis tests and interval estimates
do not end here. Say we wanted to test the hypothesis that Coffee was unrelated to Heart damage
in the \emph{simple} regression ignoring Stress.  The (Heart, Coffee) panel in \figref{fig:vis-reg-coffee11}
showed the strong marginal relation.  This can be seen in \figref{fig:vis-reg-coffee13} as
the oblique projection of the confidence ellipse to the horizontal axis where $\beta_{\mathrm{Stress}}=0$.
The estimated slope for Coffee in the simple regression is exactly the oblique shadow of
the center of the ellipse $(\widehat{\beta}_{\mathrm{Coffee}}, \widehat{\beta}_{\mathrm{Stress}})$
through the point where the ellipse has a horozontal tangent onto the horizontal axis at
$\beta_{\mathrm{Stress}}=0$. The thick blue line in this figure shows the confidence interval
for the slope for Coffee in the simple regression model. It doesn't cover the origin, so 
we reject $H_0: \beta_{\mathrm{Coffee}} = 0$ in the simple regression model.
The oblique shadow of the red 95\% confidence interval ellipse onto the horizontal axis
is slightly smaller.  How much smaller is a function of the size of the coefficient for Stress.

We can go further.  As we noted earlier, all linear combinations of variables or parameters
in data or models correspond graphically to projections (shadows) within certain sub-spaces.
Let's assume that Coffee and Stress were measured on commensurable scales, so it makes sense
to ask if they have equal impacts on Heart disease, in the joint model that includes them both.
\figref{fig:vis-reg-coffee13} also shows an auxilliary axis through the origin with slope $=-1$
corresponding to values of $\beta_{\mathrm{Stress}} - \beta_{\mathrm{Coffee}}$. The orthogonal
projection of the coefficient vector on this axis is the point estimate of 
$\widehat{\beta}_{\mathrm{Stress}} - \widehat{\beta}_{\mathrm{Coffee}}$
and the shadow of the red ellipse along this axis is the 95\% confidence interval
for the difference in slopes. This interval excludes 0, so we would reject the hypothesis
that Coffee and Stress have equal coefficients.

\input{measerror}
\input{avplot}


%\subsection{Collinearity: VIF and GVIF}
%\todo{Is this section useful?}

\input{MLM}

\input{kiss} 
%\begin{itemize*}
%\item Ridge regression
%\item BLUEs and BLUPs as kissing ellipsoids
%\item Bayesian models
%\end{itemize*}

\input{discrim}
\input{ridge}
\input{ridge2}

%\subsection{Bayesian models}
\input{bayesian}
%\subsection{Mixed models: BLUEs and BLUPs}
\input{mixed}
\input{hsbmix}

\input{conclusions}

\section{Supplementary materials}
All figures in this paper were constructed with either SAS or R software.  
The SAS examples use a collection of SAS macros from \url{http://datavis.ca/sasmac}; the
R examples employ a variety of R packages available from the CRAN web site, \url{http://cran.us.r-project.org/}
and the R-Forge development server at \url{https://r-forge.r-project.org/}.
SAS and R scripts to generate many of the figures are included as supplementary materials for this article.


\section{Acknowledgments}
This work is supported by Grant OGP0138748 from the National Sciences and Engineering Research Council of Canada.


%\bibliography{timeref,graphics}
\bibliography{references}        % processed by aux2bib
\end{document}
