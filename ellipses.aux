\relax 
\bibstyle{abbrvnat-apa-nourl}
\citation{Humboldt:1811a}
\citation{Galton:1886}
\citation{Pearson:1920}
\citation{Pearson:1896}
\citation{Pearson:1901,Hotelling:1933}
\citation{Bravais:1846}
\citation{Pearson:1920}
\citation{Denis:2001}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{Boyer:91}
\@writefile{toc}{\contentsline {section}{\numberline {2}Notation and basic results}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Geometrical ellipsoids}{2}}
\newlabel{eq:ellisoid1}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Galton's 1886 diagram, showing the relation of height of children to the average of their parents' height. The diagram is essentially an overlay of a geometrical interpretation on a bivariate grouped frequency distribution, shown as numbers.}}{3}}
\newlabel{fig:galton-corr}{{1}{3}}
\newlabel{eq:ellisoid2}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Some properties of geometric ellipsoids. Principal axes of an ellipsoid are given by the eigenvectors of $\ensuremath  {\bm  {C}}$, with radii $\sqrt  {\lambda _i}$. For a standard unit ellipsoid defined by Eqn.\nobreakspace  {}(1\hbox {}), the comparable ellipsoid for $2\ensuremath  {\bm  {C}}$ has radii multiplied by $\sqrt  {2}$. The ellipsoid for $\ensuremath  {\bm  {C}}^{-1}$ has the same principle axes, but with radii $1/\sqrt  {\lambda _i}$, making it small in the directions where $\ensuremath  {\bm  {C}}$ is large and vice-versa. }}{4}}
\newlabel{fig:inverse}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Statistical ellipsoids}{5}}
\newlabel{eq:ellipsoid3}{{3}{5}}
\newlabel{eq:ellipsoidW}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Conjugate axes and inner product spaces}{5}}
\newlabel{sec:conjugate}{{2.3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Conjugate axes of an ellipsoid with various factorizations of $\ensuremath  {\bm  {W}}$ and corresponding basis vectors. The conjugate vectors lie on the ellipsoid, and their tangents can be extended to form a parallelogram framing it. (a) Left: for an arbitrary factorization, given in Eqn.\nobreakspace  {}(6\hbox {}). (b) Right: for the Choleski factorization (green) and the principal component factorization (brown). }}{6}}
\newlabel{fig:conjugate}{{3}{6}}
\newlabel{eq:fac1}{{6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Ellipsoids in a generalized metric space}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Left: Ellipses for \ensuremath  {\bm  {H}} and \ensuremath  {\bm  {E}} in Euclidean ``data space''. Right: Ellipses for $\ensuremath  {\bm  {H}}^\star $ and $\ensuremath  {\bm  {E}}^\star $ in the transformed ``canonical space'', with the eivenvectors of\ensuremath  {\bm  {H}} relative to \ensuremath  {\bm  {E}} shown as blue arrows, whose radii are the corresponding eigenvalues, $\lambda _1, \lambda _2$. }}{7}}
\newlabel{fig:ellipse-geneig}{{4}{7}}
\citation{Monette:90}
\citation{Dempster:69}
\citation{Galton:1886}
\citation{Stigler:1986}
\citation{Monette:90}
\citation{Friendly:91}
\@writefile{toc}{\contentsline {section}{\numberline {3}The data ellipse and ellipsoids}{8}}
\newlabel{sec:data-ellipse}{{3}{8}}
\citation{Dempster:69,Monette:90}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sunflower plot of Galton's data on heights of parents and their children (in.), with 40\%, 68\% and 95\% data ellipses and the regression lines of $y$ on $x$ (black) and $x$ on $y$ (grey). The ratio of the vertical to the regression line (labeled `r') to the vertical to the top of the ellipse gives a visual estimate of the correlation ($r$=0.46, here). Shadows (projections) on the coordinate axes give standard intervals, $\mathaccentV {bar}016{x}_i \pm s_i$, with various coverage properties. Plotting children's height on the abscissa follows Galton. }}{9}}
\newlabel{fig:galton-reg3}{{5}{9}}
\newlabel{eq:dsq}{{7}{9}}
\newlabel{eq:ellipsoidS}{{8}{9}}
\citation{Anderson:35}
\citation{Fisher:36}
\citation{GnanadesikanKettenring:72}
\citation{RousseeuwLeroy:87,RousseeuwVanDriessen:99}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Scatterplot matrices of Anderson's iris data: (a) showing data, separate 68\% data ellipses and regression lines for each species; (b) showing only ellipses and regression lines. Key-- \emph  {Iris setosa}: blue, $\triangle $s; \emph  {Iris versicolor}: red, $+$; \emph  {Iris virginca}: green, $\Box $.}}{10}}
\newlabel{fig:scatirisd1}{{6}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Robust data ellipsoids}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Linear models: data ellipses and confidence ellipses}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Simple linear regression}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Annotated standard data ellipse showing standard deviations of $x$ and $y$, residual standard deviation ($s_e$), slope ($b$), and correlation ($r$). }}{11}}
\newlabel{fig:ellipses-demo}{{7}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Visualizing a confidence interval for the slope}{12}}
\newlabel{eq:ci-approx}{{9}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visual 95\% confidence interval for the slope in linear regression. Left: Standard data ellipse surrounded by the regression parallelogram. Right: Shrinking the diagonal lines by a factor of $2/\sqrt  {n}$, giving the approximate 95\% confidence interval for $\beta $.}}{12}}
\newlabel{vis-reg-prestige}{{8}{12}}
\citation{Simpson:51}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Simpson's paradox, marginal and conditional relations}{13}}
\newlabel{sec:simpson-iris}{{4.3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Marginal (a), conditional (b), and pooled within-sample (c) relations of Sepal length and Sepal width in the iris data. Total sample data ellipses are shown as black, solid curves; individual group data and ellipses are shown with colors and dashed lines}}{13}}
\newlabel{fig:contiris3}{{9}{13}}
\newlabel{eq:Sp}{{10}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Other paradoxes and fallacies}{14}}
\citation{Robinson:1950}
\citation{Robinson:1950}
\citation{Freedman:01}
\citation{Guerry:1833}
\citation{Friendly:07:guerry}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Paradoxes and fallacies: between (ecological), within (conditional) and pooled (marginal) associatons. In both panels, five groups have the same group means, $\ensuremath  {\mathsf  {Var}}(x)=6$, and $\ensuremath  {\mathsf  {Var}}(y)=2$ within each group. In the left panel, the within-group correlation is $r = +0.87$ in all groups and is $r = -0.87$ in the right panel. The green ellipse shows the average within group data ellipse.}}{15}}
\newlabel{fig:between-within}{{10}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Visual demonstration that $\ensuremath  {\bm  {\beta }}_{\textrm  {pooled}}$ lies between $\ensuremath  {\bm  {\beta }}_{\textrm  {within}}$ and $\ensuremath  {\bm  {\beta }}_{\textrm  {between}}$. Each panel shows an HE plot for the MANOVA model $(x, y) \sim \textrm  {group}$, in which the within and between ellipses are identical to those in Figure\nobreakspace  {}10\hbox {}, except for scale.}}{16}}
\newlabel{fig:between-HE}{{11}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Leverage, influence \emph  {and} precision}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Leverage-Influence quartet with data ellipses. (a) Original data; (b) Adding one low leverage outlier; (c) Adding one ``good'' leverage point; (d) Adding one ``bad'' leverage point. In panels (b)--(d) the dashed black line is the fitted line for the original data, while the thick solid blue line reflects the regression including the additional point. The data ellipses show the effect of the additional point on precision.}}{17}}
\newlabel{fig:levdemo21}{{12}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Data ellipses in the Leverage-Influence quartet. This graph overlays the data ellipses and additional points from the four panels of Figure\nobreakspace  {}12\hbox {}. It can be seen that only the OL point affects the slope, while the O and L points affect precision of the estimates in opposite directions.}}{18}}
\newlabel{fig:levdemo22}{{13}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Ellipsoids in data space and beta space}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Scatterplot matrix, showing the relations between Heart ($y$), Coffee ($x_1$) and Stress ($x_2$), with linear regression lines and 68\% data ellipses for the marginal bivariate relations. }}{19}}
\newlabel{fig:vis-reg-coffee11}{{14}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Data space and $\ensuremath  {\bm  {\beta }}$ space representations of Coffee and Stress. Left: Standard (40\%) data ellipse Right: Joint 95\% confidence ellipse (green) for ($\beta _{\mathrm  {Coffee}}, \beta _{\mathrm  {Stress}}$), CI ellipse (red) with 95\% univariate shadows. }}{20}}
\newlabel{fig:vis-reg-coffee12}{{15}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Joint 95\% confidence ellipse for ($\beta _{\mathrm  {Coffee}}, \beta _{\mathrm  {Stress}}$), together with the 1D marginal confidence interval for $\beta _{\mathrm  {Coffee}}$ ignoring Stress (thick blue line), and a visual confidence interval for $\beta _{\mathrm  {Stress}} - \beta _{\mathrm  {Coffee}}=0$ (dark cyan). }}{22}}
\newlabel{fig:vis-reg-coffee13}{{16}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Measurement error}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Effects of measurement error in Stress on the marginal relation between Heart disease and Stress. Each panel starts with the observed data ($\delta =0$), then adds random normal error, $\mathcal  {N}(0, \delta \times SD_{Stress})$, with $\delta = \{0.75, 1.0, 1.5\}$ to the value of Stress. Increasing measurement error biases the slope for Stress toward 0. Left: 50\% data ellipses; right: 50\% confidence ellipses for $(\beta _0, \beta _{Stress})$. }}{23}}
\newlabel{fig:coffee-stress}{{17}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Ellipsoids in added variable plots}{23}}
\newlabel{sec:avp}{{4.8}{23}}
\citation{VellemanWelsh:81}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Added variable plots for Stress and Coffee in the multiple regression predicting Heart disease. Each panel also shows the 50\% conditional data ellipse for partial residuals, $(\ensuremath  {\bm  {x}}_k^\star , \ensuremath  {\bm  {y}}^\star )$, shaded red. }}{24}}
\newlabel{fig:coffee-avplot-A}{{18}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Added variable $+$ marginal plots for Stress and Coffee in the multiple regression predicting Heart disease. Each panel shows the 50\% conditional data ellipse for partial residuals (shaded, red) as well as the marginal 50\% data ellipse for the $(x, y)$ variables, shifted to the origin. Arrows connect the mean-centered marginal points (open circles) to the partial residual points (filled circles).}}{25}}
\newlabel{fig:coffee-avplot-B}{{19}{25}}
\citation{Timm:75}
\@writefile{toc}{\contentsline {section}{\numberline {5}Multivariate linear models: HE plots}{26}}
\newlabel{sec:mlm}{{5}{26}}
\newlabel{eq:mglt}{{11}{26}}
\newlabel{eq:hmat}{{12}{26}}
\newlabel{eq:emat}{{13}{26}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Multivariate test statistics as functions of the eigenvalues $\lambda _i$ solving $\ensuremath  {\mathrm  {det} (\ensuremath  {\bm  {H}} - \lambda \ensuremath  {\bm  {E}})}=0$ or eigenvalues $\rho _i$ solving $\ensuremath  {\mathrm  {det} (\ensuremath  {\bm  {H}} - \rho (\ensuremath  {\bm  {H}}+\ensuremath  {\bm  {E}}))}=0$ }}{27}}
\newlabel{tab:criteria}{{1}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Hypothesis-Error (HE) plots}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces (a) Data ellipses and (b) corresponding HE plot for sepal length and petal length in the Iris dataset. The \ensuremath  {\bm  {H}} ellipse is the data ellipse of the fitted values defined by the group means, $\mathaccentV {bar}016{\ensuremath  {\bm  {y}}}_{i \cdot }$ The \ensuremath  {\bm  {E}} ellipse is the data ellipse of the residuals, $(\ensuremath  {\bm  {y}}_{ij} - \mathaccentV {bar}016{\ensuremath  {\bm  {y}}}_{i \cdot })$. Using evidence (``significance'') scaling of the \ensuremath  {\bm  {H}} ellipse, the plot has the property that the multivariate test for a given hypothesis is significant by Roy's largest root test \emph  {iff} the \ensuremath  {\bm  {H}} ellipse protrudes anywhere outside the \ensuremath  {\bm  {E}} ellipse.}}{28}}
\newlabel{fig:heplot3a}{{20}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Linear hypotheses: Geometries of contrasts and sums of effects}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces $\ensuremath  {\bm  {H}}$ and $\ensuremath  {\bm  {E}}$ matrices for sepal width and sepal length in the iris data, together with $\ensuremath  {\bm  {H}}$ matrices for testing two orthogonal contrasts in the species effect.}}{29}}
\newlabel{fig:HE-contrasts-iris}{{21}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Canonical projections: ellipses in data space and canonical space}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Canonical HE plot for the Iris data. In this plot, the \ensuremath  {\bm  {H}} ellipse is shown using effect-size scaling to preserve resolution, and the variable vectors have been multiplied by a constant to approximately fill the plot space. The projections of the variable vectors on the coordinate axes show the correlations of the variables with the canonical dimensions.}}{31}}
\newlabel{fig:HE-can-iris}{{22}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Kissing ellipsoids}{32}}
\newlabel{eq:kiss-demoA}{{15}{32}}
\newlabel{eq:locus}{{16}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Locus of osculation for two ellipsoidal level curves, with centers at $\ensuremath  {\bm  {m}}_1 = (-2, 2)$ and $\ensuremath  {\bm  {m}}_2 = (2, 6)$ and shape matrices $\ensuremath  {\bm  {A}}_1$ and $\ensuremath  {\bm  {A}}_2$ given in Eqn.\nobreakspace  {}(15\hbox {}). The left ellipsoids (red) have radii=1, 2, 3. The right ellipsoids have radii=1, 1.74, 3.1, where the last two values were chosen to make them kiss at the points marked with squares. The black curve is an approximation to the path of osculation, using a spline function connecting $\ensuremath  {\bm  {m}}_1$ to $\ensuremath  {\bm  {m}}_2$ via the marked points of osculation.}}{33}}
\newlabel{fig:kiss-demo}{{23}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Discriminant analysis}{33}}
\citation{Fox:2008}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Locus of osculation for two ellipsoidal level curves, showing contour lines of the vector cross product function Eqn.\nobreakspace  {}(16\hbox {}). The thick black curve shows the complete locus of osculation for these two ellipses, where the cross product function equals 0. Left: with parameters as in Figure\nobreakspace  {}23\hbox {} and Eqn.\nobreakspace  {}(15\hbox {}). Right: with the same shape matrix, $\ensuremath  {\bm  {A}}_1$ for both ellipsoids.}}{34}}
\newlabel{fig:kiss-demo2}{{24}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ridge regression}{34}}
\newlabel{eq:ridgeRSS}{{18}{34}}
\citation{HoerlKennard:1970a,HoerlKennard:1970b}
\citation{Marquardt:1970}
\newlabel{eq:ridge-beta}{{19}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Elliptical contours of the OLS residual sum of squares for two parameters in a regression, together with circular contours for the constraint function, $\beta _1^2 + \beta _2^2 \le t$. Ridge regression finds the point $\ensuremath  {\bm  {\beta }}^{RR}$ where the OLS contours just kiss the contstraint region.}}{35}}
\newlabel{fig:ridge-demo}{{25}{35}}
\newlabel{eq:ridge}{{20}{35}}
\citation{HoerlKennard:1970b}
\citation{Marquardt:1970}
\citation{Longley:1967}
\citation{Longley:1967}
\newlabel{eq:ridge-sup1}{{21}{36}}
\newlabel{eq:ridge-sup2}{{22}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Bivariate ridge trace plots}{36}}
\newlabel{sec:ridge2}{{6.2.1}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Bivariate ridge trace plots for the coefficients of four predictors against the coefficient for GNP in Longley's data, with $k = 0, 0.005, 0.01, 0.02, 0.04, 0.08$. In most cases the coefficients are driven toward zero, but the bivariate plot also makes clear the reduction in variance. To reduce overlap, all confidence ellipses are shown with 1/2 the standard radius.}}{37}}
\newlabel{fig:ridge2}{{26}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Bayesian linear models}{38}}
\newlabel{eq:bayes-qform}{{24}{38}}
\citation{BrykRaudenbush:1992,RaudenbushBryk:2002}
\citation{RaudenbushBryk:2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Mixed models: BLUEs and BLUPs}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Example: Math achievement and SES}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Conclusions}{39}}
\citation{Galton:1886}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Comparing BLUEs and BLUPs. Each panel plots the OLS estimates from separate regressions for each school (BLUEs) versus the mixed model estimates from the random intercepts and slopes model (BLUPs). Left: intercepts; Right: slopes for CSES. The shrinkage of the BLUPs toward the OLS estimate is much greater for slopes than intercepts. }}{40}}
\newlabel{fig:hsbmix4}{{27}{40}}
\citation{Dempster:69}
\citation{Cramer:1946}
\citation{Pearson:1901,Hotelling:1933}
\citation{Galton:1886}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Comparing BLUEs and BLUPs. The plot shows ellipses of 50\% coverage for the estimates of intercepts and slopes from OLS regressions (BLUEs) and the mixed model (BLUPs), separately for each sector. The centers of the ellipses illustrate how the BLUPS can be considered a weighted average of the BLUEs and the pooled OLS estimate, ignoring sector. The relative sizes of the ellipses reflect the smaller variance for the BLUPs compared to the BLUEs, particularly for slope estimates. }}{41}}
\newlabel{fig:hsbmix43}{{28}{41}}
\citation{Beaton:64}
\citation{Monette:90}
\citation{car,heplots1}
\citation{Friendly:07:manova}
\@writefile{toc}{\contentsline {section}{\numberline {8}Supplementary materials}{42}}
\bibdata{timeref,graphics,Rpackages}
\bibcite{Anderson:35}{{1}{1935}{{Anderson}}{{}}}
\bibcite{Beaton:64}{{2}{1964}{{Beaton}}{{}}}
\bibcite{Boyer:91}{{3}{1991}{{Boyer}}{{}}}
\bibcite{Bravais:1846}{{4}{1846}{{Bravais}}{{}}}
\bibcite{BrykRaudenbush:1992}{{5}{1992}{{Bryk and Raudenbush}}{{}}}
\bibcite{Cramer:1946}{{6}{1946}{{Cram{\'e}r}}{{}}}
\bibcite{Dempster:69}{{7}{1969}{{Dempster}}{{}}}
\bibcite{Denis:2001}{{8}{2001}{{Denis}}{{}}}
\bibcite{Fisher:36}{{9}{1936}{{Fisher}}{{}}}
\bibcite{Fox:2008}{{10}{2008}{{Fox}}{{}}}
\bibcite{car}{{11}{2009}{{Fox}}{{}}}
\bibcite{heplots1}{{12}{2009}{{Fox \emph  {et~al.}}}{{Fox, Friendly, and Monette}}}
\bibcite{Freedman:01}{{13}{2001}{{Freedman}}{{}}}
\bibcite{Friendly:91}{{14}{1991}{{Friendly}}{{}}}
\bibcite{Friendly:07:guerry}{{15}{2007{a}}{{Friendly}}{{}}}
\bibcite{Friendly:07:manova}{{16}{2007{b}}{{Friendly}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Acknowledgments}{43}}
\bibcite{Galton:1886}{{17}{1886}{{Galton}}{{}}}
\bibcite{GnanadesikanKettenring:72}{{18}{1972}{{Gnanadesikan and Kettenring}}{{}}}
\bibcite{Guerry:1833}{{19}{1833}{{Guerry}}{{}}}
\bibcite{HoerlKennard:1970b}{{20}{1970{a}}{{Hoerl and Kennard}}{{}}}
\bibcite{HoerlKennard:1970a}{{21}{1970{b}}{{Hoerl and Kennard}}{{}}}
\bibcite{Hotelling:1933}{{22}{1933}{{Hotelling}}{{}}}
\bibcite{Longley:1967}{{23}{1967}{{Longley}}{{}}}
\bibcite{Marquardt:1970}{{24}{1970}{{Marquardt}}{{}}}
\bibcite{Monette:90}{{25}{1990}{{Monette}}{{}}}
\bibcite{Pearson:1896}{{26}{1896}{{Pearson}}{{}}}
\bibcite{Pearson:1901}{{27}{1901}{{Pearson}}{{}}}
\bibcite{Pearson:1920}{{28}{1920}{{Pearson}}{{}}}
\bibcite{RaudenbushBryk:2002}{{29}{2002}{{Raudenbush and Bryk}}{{}}}
\bibcite{Robinson:1950}{{30}{1950}{{Robinson}}{{}}}
\bibcite{RousseeuwLeroy:87}{{31}{1987}{{Rousseeuw and Leroy}}{{}}}
\bibcite{RousseeuwVanDriessen:99}{{32}{1999}{{Rousseeuw and Van~Driessen}}{{}}}
\bibcite{Simpson:51}{{33}{1951}{{Simpson}}{{}}}
\bibcite{Stigler:1986}{{34}{1986}{{Stigler}}{{}}}
\bibcite{Timm:75}{{35}{1975}{{Timm}}{{}}}
\bibcite{VellemanWelsh:81}{{36}{1981}{{Velleman and Welsh}}{{}}}
\bibcite{Humboldt:1811a}{{37}{1811}{{von Humboldt}}{{}}}
