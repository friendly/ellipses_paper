\subsection{Bayesian linear models}\label{sec:bayesian}

In a Bayesian alternative to standard least squares estimation, consider the case where our prior
information about \vec{\beta} can be encapsulated in a distribution with a prior mean
$\vec{\beta}^{\mathrm{prior}}$ and covariance matrix \mat{A}.  We show that under reasonable conditions
the Bayesian posterior
estimate, $\widehat{\vec{\beta}}^{\mathrm{posterior}}$, turns out
to be a weighted average of the prior coefficients $\vec{\beta}^{\mathrm{prior}}$ and the OLS solution $\widehat{\vec{\beta}}^{\mathrm{OLS}}$,
with weights proportional to the conditional prior precision, $\mat{A}^{-1}$, and the data precision given by
$\mat{X}\trans\mat{X}$.   Once again, this can be understood geometrically as the locus of osculation of
ellipsoids that characterize the prior and the data.

Under Gaussian assumptions,
the conditional likelihood can be written as
\begin{equation*}
\mathcal{L} (\vec{y} \given \mat{X}, \vec{\beta}, \sigma^2) \propto
   (\sigma^2)^{-n/2} \; \exp \left[ -\frac{1}{2 \sigma^2} (\vec{y}-\mat{X} \vec{\beta}) \trans  (\vec{y}-\mat{X} \vec{\beta})   \right] \period
\end{equation*}
To focus on alternative estimators, we can complete the square around $\widehat{\vec{\beta}} = \widehat{\vec{\beta}}^{\mathrm{OLS}}$ to give

\begin{equation} \label{eq:bayes-qform}
(\vec{y}- \mat{X} \vec{\beta})\trans(\vec{y}- \mat{X} \vec{\beta}) =
(\vec{y}- \mat{X} \widehat{\vec{\beta}})\trans(\vec{y}- \mat{X} \widehat{\vec{\beta}}) +
(\vec{\beta} - \widehat{\vec{\beta}})\trans(\mat{X}\trans\mat{X})(\vec{\beta} - \widehat{\vec{\beta}}) \period
\end{equation}
With a little manipulation, a conjugate prior, of the form $\Pr (\vec{\beta}, \sigma^2) = \Pr (\vec{\beta} \given \sigma^2) \times \Pr (\sigma^2)$
can be expressed with $\Pr (\sigma^2)$ an inverse gamma distribution depending on the first term on the right hand side of \eqref{eq:bayes-qform}
and $\Pr (\vec{\beta} \given \sigma^2)$ a normal distribution,
\begin{equation}
\Pr (\vec{\beta} \given \sigma^2) \propto (\sigma^2)^{-p}  \times \exp \left[ -\frac{1}{2 \sigma^2}  (\vec{\beta} - \vec{\beta}^{\mathrm{prior}}) \trans  \: \mat{A} (\vec{\beta} - \vec{\beta}^{\mathrm{prior}})  \right] \period
\end{equation}

The posterior distribution is then
$\Pr (\vec{\beta}, \sigma^2 \given \vec{y}, \mat{X}) \propto \Pr (\vec{y} \given \mat{X}, \vec{\beta}, \sigma^2) \times
\Pr (\vec{\beta} \given \sigma^2) \times \Pr (\sigma^2)$,
whence, after some simplification,
the posterior mean can be expressed as

\begin{equation}\label{eq:bayes-posterior}
\widehat{\vec{\beta}}^{\mathrm{posterior}} =
(\mat{X}\trans \mat{X}\widehat{\vec{\beta}}^{\mathrm{OLS}} + \mat{A} \vec{\beta}^{\mathrm{prior}})
(\mat{X}\trans \mat{X}+\mat{A})^{-1}
\end{equation}
with covariance matrix $(\mat{X}\trans \mat{X}+\mat{A})^{-1}$. The posterior coefficients are thus a weighted average of
the prior coefficients and the OLS estimates, with weights given by the conditional prior precision, $\mat{A}^{-1}$,
and the data precision, $\mat{X}\trans\mat{X}$.  Thus, as we increase the strength of our prior precision (decreasing
prior variance), we place greater weight on our prior beliefs relative to the data.

In this context, ridge regression can be seen as the special case where $\widehat{\vec{\beta}}^{\mathrm{prior}} = \vec{0}$ and $\mat{A} = k \mat{I}$
and where \figref{fig:ridge-demo} provides an elliptical visualization. In \eqref{eq:ridge-sup2}, the number of observations,
$n(k)$ corresponding to $\mat{X}_k^0$ can be seen as another way of expressing the weight of the prior in relation to the data.

% \TODO{Complete this section.  What have I omitted in the development, necessary for this paper?
% Are there other geometric insights?  Does it need another, Bayesian specific graph?
% Is there a simple expression for $\Var({\widehat{\vec{\beta}}^{\mathrm{posterior}}})$
% that could be exploited?
% }



